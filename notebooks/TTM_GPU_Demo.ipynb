{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyTimeMixers (TTM) GPU Demo\n",
    "\n",
    "This notebook demonstrates the TinyTimeMixers implementation on GPU.\n",
    "\n",
    "**Paper:** arXiv 2401.03955 - Tiny Time Mixers (TTM): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/evelynmitchell/TinyTimeMixers/blob/main/notebooks/TTM_GPU_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the TinyTimeMixers package and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TinyTimeMixers from GitHub\n",
    "!pip install git+https://github.com/evelynmitchell/TinyTimeMixers.git\n",
    "\n",
    "# Or install dependencies manually if developing locally\n",
    "# !pip install torch numpy einops tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "TinyTimeMixers (TTM) is a lightweight time series foundation model based on the TSMixer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinytimemixers import TTM, TTMConfig\n",
    "\n",
    "# Default configuration\n",
    "config = TTMConfig()\n",
    "print(\"TTM Configuration:\")\n",
    "print(f\"  Context length: {config.context_length}\")\n",
    "print(f\"  Prediction length: {config.prediction_length}\")\n",
    "print(f\"  Patch length: {config.patch_length}\")\n",
    "print(f\"  Hidden features: {config.hidden_features}\")\n",
    "print(f\"  Backbone levels: {config.num_backbone_levels}\")\n",
    "print(f\"  Blocks per level: {config.blocks_per_level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = TTM(config, num_channels=7).to(device)\n",
    "print(\"\\nModel created!\")\n",
    "print(f\"Total parameters: {model.get_num_parameters():,}\")\n",
    "\n",
    "# Parameter breakdown\n",
    "print(\"\\nParameter breakdown:\")\n",
    "for name, count in model.get_parameter_breakdown().items():\n",
    "    print(f\"  {name}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Demo\n",
    "\n",
    "Let's test the model with random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random input\n",
    "batch_size = 32\n",
    "num_channels = 7\n",
    "context_length = config.context_length\n",
    "\n",
    "x = torch.randn(batch_size, num_channels, context_length).to(device)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(\n",
    "    f\"\\nExpected: (batch={batch_size}, channels={num_channels}, pred_len={config.prediction_length})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Speed Benchmark\n",
    "\n",
    "Compare CPU vs GPU inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, x, num_runs=100, warmup=10):\n",
    "    \"\"\"Benchmark inference speed.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "\n",
    "    # Sync CUDA if using GPU\n",
    "    if x.is_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(x)\n",
    "            if x.is_cuda:\n",
    "                torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    return elapsed / num_runs * 1000  # ms per inference\n",
    "\n",
    "\n",
    "# Benchmark on current device\n",
    "batch_size = 1\n",
    "x = torch.randn(batch_size, 7, config.context_length).to(device)\n",
    "\n",
    "ms_per_inference = benchmark_inference(model, x)\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Inference time: {ms_per_inference:.2f} ms\")\n",
    "print(f\"Throughput: {1000/ms_per_inference:.1f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark with different batch sizes\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Batch size vs Throughput:\\n\")\n",
    "    for bs in [1, 4, 16, 32, 64, 128]:\n",
    "        try:\n",
    "            x = torch.randn(bs, 7, config.context_length).to(device)\n",
    "            ms = benchmark_inference(model, x, num_runs=50)\n",
    "            throughput = bs * 1000 / ms\n",
    "            print(f\"  Batch {bs:3d}: {ms:6.2f} ms, {throughput:7.1f} samples/sec\")\n",
    "        except RuntimeError:\n",
    "            print(f\"  Batch {bs:3d}: OOM\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Example\n",
    "\n",
    "Simple training loop example with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "def generate_synthetic_data(num_samples, num_channels, context_len, pred_len):\n",
    "    \"\"\"Generate synthetic time series with trend and seasonality.\"\"\"\n",
    "    t = np.linspace(0, 4 * np.pi, context_len + pred_len)\n",
    "\n",
    "    X, Y = [], []\n",
    "    for _ in range(num_samples):\n",
    "        # Random parameters\n",
    "        amplitude = np.random.uniform(0.5, 2.0, num_channels)\n",
    "        phase = np.random.uniform(0, 2 * np.pi, num_channels)\n",
    "        trend = np.random.uniform(-0.1, 0.1, num_channels)\n",
    "\n",
    "        # Generate time series\n",
    "        series = []\n",
    "        for c in range(num_channels):\n",
    "            s = amplitude[c] * np.sin(t + phase[c]) + trend[c] * t\n",
    "            s += np.random.randn(len(t)) * 0.1  # noise\n",
    "            series.append(s)\n",
    "        series = np.stack(series, axis=0)\n",
    "\n",
    "        X.append(series[:, :context_len])\n",
    "        Y.append(series[:, context_len:])\n",
    "\n",
    "    return torch.FloatTensor(np.stack(X)), torch.FloatTensor(np.stack(Y))\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X_train, Y_train = generate_synthetic_data(\n",
    "    1000, 7, config.context_length, config.prediction_length\n",
    ")\n",
    "X_val, Y_val = generate_synthetic_data(\n",
    "    100, 7, config.context_length, config.prediction_length\n",
    ")\n",
    "\n",
    "print(f\"Training data: X={X_train.shape}, Y={Y_train.shape}\")\n",
    "print(f\"Validation data: X={X_val.shape}, Y={Y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Reset model\n",
    "model = TTM(config, num_channels=7).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "print(f\"Training on {device}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "            Y_pred = model(X_batch)\n",
    "            val_loss += criterion(Y_pred, Y_batch).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:2d}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a sample prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_x = X_val[0:1].to(device)\n",
    "    sample_y_true = Y_val[0:1]\n",
    "    sample_y_pred = model(sample_x).cpu()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    channel = i\n",
    "\n",
    "    # Context\n",
    "    context = sample_x[0, channel].cpu().numpy()\n",
    "    ax.plot(range(len(context)), context, \"b-\", label=\"Context\")\n",
    "\n",
    "    # True future\n",
    "    true = sample_y_true[0, channel].numpy()\n",
    "    ax.plot(\n",
    "        range(len(context), len(context) + len(true)),\n",
    "        true,\n",
    "        \"g-\",\n",
    "        label=\"True\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    # Predicted\n",
    "    pred = sample_y_pred[0, channel].numpy()\n",
    "    ax.plot(\n",
    "        range(len(context), len(context) + len(pred)),\n",
    "        pred,\n",
    "        \"r--\",\n",
    "        label=\"Predicted\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    ax.axvline(x=len(context), color=\"k\", linestyle=\":\", alpha=0.5)\n",
    "    ax.set_title(f\"Channel {channel}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"ttm_trained.pt\")\n",
    "print(\"Model saved to ttm_trained.pt\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = TTM.load(\"ttm_trained.pt\")\n",
    "loaded_model = loaded_model.to(device)\n",
    "print(f\"Model loaded. Parameters: {loaded_model.get_num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Forward pass\n",
    "    x = torch.randn(64, 7, config.context_length).to(device)\n",
    "    y = model(x)\n",
    "\n",
    "    allocated = torch.cuda.max_memory_allocated() / 1e6\n",
    "    print(f\"Peak GPU memory: {allocated:.1f} MB\")\n",
    "\n",
    "    # Training pass\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, torch.randn_like(y_pred))\n",
    "    loss.backward()\n",
    "\n",
    "    allocated = torch.cuda.max_memory_allocated() / 1e6\n",
    "    print(f\"Peak GPU memory (training): {allocated:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading and configuring TTM model\n",
    "2. Forward pass and inference benchmarking\n",
    "3. Training loop with synthetic data\n",
    "4. Visualization of predictions\n",
    "5. Model saving/loading\n",
    "\n",
    "For production use, you would:\n",
    "- Use real time series datasets (Monash, GIFT-Eval)\n",
    "- Implement proper train/val/test splits\n",
    "- Add learning rate scheduling\n",
    "- Use the TTMForFinetune class with frozen backbone"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
